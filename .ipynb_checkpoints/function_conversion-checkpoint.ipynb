{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540b7c3-a1ca-49cf-b678-34365492dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for converting code into functions, then .py files \n",
    "\n",
    "# functionalize the \"debug arc\"/infeasible node removal process\n",
    "# further distinguish function arg names from intermediate object names\n",
    "# finish filling in function info\n",
    "# clean up intro stuff\n",
    "# do a test run (very likely there are name errors)\n",
    "\n",
    "# once it runs: export to .py files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c02c13-1ab3-4156-bae6-680c59478108",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de697964-f9dc-4423-9f1e-44cd93d0aa42",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd2460ce-6787-48f5-ba48-d45d36c70463",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddc2a6-cad5-482f-ac9d-1b6779814f36",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeef87c5-689a-435d-90de-a0f8a5f0aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexington-Fayette-Urban County Government Urban Service Area\n",
    "# https://hub.arcgis.com/datasets/4c8db8e014cb49349fd430d96d8994b9_0/about\n",
    "city_boundary_gdf = (\n",
    "    gpd.read_file('data/lfucg_usb/Urban_Service_Area.shp')\n",
    "    .to_crs('EPSG:4326') # project to classic CRS\n",
    ")\n",
    "city_boundary=city_boundary_gdf.geometry.iloc[0]\n",
    "\n",
    "# list of census blocks to remove\n",
    "census_block_exclude_list = [\n",
    "    '210670040061008', # rural; 770m from any bike network nodes; 44 ppl \n",
    "    '210670038023034', # connection issues; 3 ppl\n",
    "    '210670039134000', # rural; >450m from any bike network nodes;\n",
    "    '210670039161008', # rural; >450m from any bike network nodes;\n",
    "    '210670039182019', # rural; >450m from any bike network nodes;\n",
    "    '210670042081006', # rural; >450m from any bike network nodes;\n",
    "    '210670040051003', # rural; >450m from any bike network nodes;\n",
    "    '210670039182039', # rural; >450m from any bike network nodes;\n",
    "    '210670039062021', # used debug arcs; try adding back later\n",
    "    '210670039062023', # used debug arcs; try adding back later\n",
    "    '210670038023033', # used debug arcs; try adding back later\n",
    "    '210670037021000', # used debug arcs; try adding back later\n",
    "    '210670037022012', # used debug arcs; try adding back later\n",
    "]\n",
    "\n",
    "# US census data\n",
    "# census block geometry\n",
    "census_blocks_gdf = (\n",
    "    gpd\n",
    "    .read_file('data/tl_2020_21067_tabblock20/tl_2020_21067_tabblock20.shp')\n",
    "    .query('GEOID20 not in @census_block_exclude_list') # remove blocks from network\n",
    "    #.assign(in_USB=lambda df: df['geometry'].within(city_boundary)) # define a truth column\n",
    "    #.query(\"in_USB == True\") # select only blocks inside the USB # moved this step to data wrangle\n",
    ")\n",
    "# population data\n",
    "population_df = (\n",
    "    pd\n",
    "    .read_csv('data/DECENNIALPL2020.H1_2024-04-02T135509/DECENNIALPL2020.H1-Data.csv')\n",
    "    .query('GEO_ID != \"Geography\"') # DECENNIALPL2020 has a \"header\" row\n",
    "    .drop_duplicates() # DECENNIALPL2020 has 101 rows that are listed twice \n",
    ")\n",
    "# https://api.census.gov/data/2020/dec/pl/variables.html for more info \n",
    "\n",
    "# list of arcs to remove from the bike network\n",
    "arc_exclude_list = [\n",
    "    83568, # disconnects census block 210670026005001\n",
    "    126616, # golf course; disconnects census block 210670019001020\n",
    "    126608, # golf course; disconnects census block 210670019001003\n",
    "    126612, # golf course; disconnects census block 210670019001002\n",
    "    60836, # parking lot; disconnects census block 210670019002002\n",
    "    60835, # parking lot; disconnects census block 210670019002002\n",
    "    62039, # parking lot; disconnects census block 210670019002002\n",
    "    79572, # parking lot; disconnects census block 210670010002002\n",
    "    85764, # parking lot; disconnects census block 210670010002002\n",
    "    116486, # sidewalk; disconnects census block 210670039084042\n",
    "    98492, # sidewalk; disconnects census block 210670039084042\n",
    "    126711, # golf course; disconnects census block 210670034052004\n",
    "    126398, # golf course; disconnects census block 210670034052003\n",
    "    81467, # parking lot; disconnects census block 210670041053003 \n",
    "    81468, # parking lot; disconnects census block 210670041053003 \n",
    "    81469, # parking lot; disconnects census block 210670041053003 \n",
    "    81471, # parking lot; disconnects census block 210670041053003 \n",
    "    81472, # parking lot; disconnects census block 210670041053003 \n",
    "    81477, # parking lot; disconnects census block 210670041053003 \n",
    "    119911, # parking lot; disconnects census block 210670041053003 \n",
    "    59819, # sidewalk; disconnects census block 210670001011027\n",
    "    57257, # sidewalk; disconnects census block 210670001011027\n",
    "    59818, # sidewalk; disconnects census block 210670001011008\n",
    "    65637, # parking lot; disconnects census block 210670003003006\n",
    "    50229, # sidewalk; disconnects census block 210670004002004\n",
    "    50111, # sidewalk; disconnects census block 210670004002004\n",
    "    59541, # sidewalk; disconnects census block 210670018002015\n",
    "    54569, # sidewalk; disconnects census block 210670018002015\n",
    "    54180, # sidewalk; disconnects census block 210670008022002\n",
    "    73268, # parking lot; disconnects census block 210670007001013\n",
    "    73296, # parking lot; disconnects census block 210670007001013\n",
    "    73294, # parking lot; disconnects census block 210670007001013\n",
    "    73292, # parking lot; disconnects census block 210670007001013\n",
    "    71562, # parking lot; disconnects census block 210670007002002\n",
    "    74034, # parking lot; disconnects census block 210670007002002\n",
    "    126440, # golf course; disconnects census block 210670017002000\n",
    "    126441, # golf course; disconnects census block 210670017002000\n",
    "    126442, # golf course; disconnects census block 210670017002000\n",
    "    126443, # golf course; disconnects census block 210670017002000\n",
    "    126444, # golf course; disconnects census block 210670017002000\n",
    "    126445, # golf course; disconnects census block 210670017002000\n",
    "    126446, # golf course; disconnects census block 210670017002000\n",
    "    126447, # golf course; disconnects census block 210670017002000\n",
    "    126448, # golf course; disconnects census block 210670017002000\n",
    "    126449, # golf course; disconnects census block 210670017002000\n",
    "    126450, # golf course; disconnects census block 210670017002000\n",
    "    81470, # parking lot; disconnects census block 210670041053003\n",
    "    83569, # parking lot; disconnects census block 210670026005001\n",
    "    83571, # parking lot; disconnects census block 210670026005001\n",
    "    83569, # parking lot; disconnects census block 210670026005001\n",
    "    62068, # parking lot; disconnects census block 210670019002002\n",
    "    62071, # parking lot; disconnects census block 210670019002002\n",
    "    60837, # parking lot; disconnects census block 210670019002002\n",
    "    50232, # sidewalk; disconnects census block 210670004002004\n",
    "    58492, # sidewalk; disconnects census block 210670004002004\n",
    "    126397, # golf course; disconnects census block 210670034052003\n",
    "    126396, # golf course; disconnects census block 210670034052003\n",
    "    126397, # golf course; disconnects census block 210670034052003\n",
    "    126710, # golf course; disconnects census block 210670034052003\n",
    "    71233, # golf course; disconnects census block 210670034052003\n",
    "    72867, # golf course; disconnects census block 210670034052003\n",
    "    72868, # golf course; disconnects census block 210670034052003\n",
    "    72869, # golf course; disconnects census block 210670034052003\n",
    "    72870, # golf course; disconnects census block 210670034052003\n",
    "    58109, # sidewalk; disconnects census block 210670001021003; maybe\n",
    "    59254, # sidewalk;disconnects census block 210670001021003; maybe\n",
    "    79919, # parking lot; disconnects census block 210670040013000; maybe\n",
    "    79918, # parking lot; disconnects census block 210670040013000; maybe\n",
    "    79921, # parking lot; disconnects census block 210670040013000; maybe\n",
    "    79917, # parking lot; disconnects census block 210670040013000; maybe\n",
    "    114815, # parking lot; disconnects census block 210670032021009; maybe\n",
    "    109752, # parking lot; disconnects census block 210670032021009; maybe\n",
    "    109753, # parking lot; disconnects census block 210670032021009; maybe\n",
    "    106337, # parking lot; disconnects census block 210670038023034; maybe\n",
    "    106339, # parking lot; disconnects census block 210670038023034; maybe\n",
    "    106338, # parking lot; disconnects census block 210670038023034; maybe\n",
    "    101567, # parking lot; disconnects census block 210670038023034; maybe\n",
    "    106340, # parking lot; disconnects census block 210670038023034; maybe\n",
    "    86396, # parking lot; disconnects census block 210670037042044; maybe\n",
    "    94649, # parking lot; disconnects census block 210670037042044; maybe\n",
    "    #86394, # parking lot; disconnects census block 210670037042044; maybe\n",
    "    #86395, # parking lot; disconnects census block 210670037042044; maybe\n",
    "]\n",
    "\n",
    "# PeopleForBikes cycle network data\n",
    "pfb_gdf = (\n",
    "    gpd.read_file('data/people_for_bikes/neighborhood_ways/neighborhood_ways.shp')\n",
    "    .query('ROAD_ID not in @arc_exclude_list') # remove disconnected edges from the network\n",
    "    .query(\"FUNCTIONAL != 'motorway'\") # remove the highways\n",
    "    .query(\"FUNCTIONAL != 'motorway_link'\") # exit ramps etc.\n",
    ")\n",
    "\n",
    "# list of amenities to exclude based on local knowledge\n",
    "# consider editing the source file directly \n",
    "dest_exclude_list = [\n",
    "    'node/12001067429', # 'The Venue Shopping Center Courtyard' is not a real park\n",
    "    'node/8520821500', # Speigle Heights Park has a node and a way\n",
    "    'node/12001059631', # Zandale Park has a node and a way\n",
    "    'node/3197373270' # Red Mile Horse training area isn't a 'park' in the sense we care about\n",
    "]\n",
    "\n",
    "# amenity data\n",
    "amenities_gdf = (\n",
    "    gpd\n",
    "    .read_file('data/lex_parks_export.geojson')\n",
    "    .cx[-84.6 : -84.3,  37.9 : 38.2] # filter non-KY Lexingtons using a bounding box\n",
    "    .query('id not in @dest_exclude_list') # remove individual entries\n",
    "    # consider adding (or replacing) this filter with the Urban Service Boundary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b343ae1-f872-4179-a27c-a7f1b866890c",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "742af53a-65b0-44de-8e74-acc4108e8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radii to connect nodes from different data sets \n",
    "radii = [50,100,150,200,250,300,400]  \n",
    "\n",
    "# coordinate reference systems\n",
    "#cycle_network_crs = 'EPSG:4326'\n",
    "distance_crs = 'ESRI:102003' # Albers contiguous USA, for distance calculations\n",
    "\n",
    "# budget as a proportion of total cost\n",
    "total_cost_proportion = 0.15\n",
    "\n",
    "# upgrade costs (in $million/mile)\n",
    "cost_per_mile = 2 * 10**6 # $2mil/mi\n",
    "cost_per_meter = cost_per_mile/1609.34\n",
    "\n",
    "# factor by which a low stress path would need to exceed a high stress route in order for someone to choose the shorter high stress route. \n",
    "# equiv: someone would be willing f times further in order to stay on a low stress path\n",
    "# see https://transweb.sjsu.edu/sites/default/files/1005-low-stress-bicycling-network-connectivity.pdf\n",
    "# page 3\n",
    "f = 1.25 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd7e96-cd31-4687-a8d1-a7162aa2e363",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3dc75-3056-4c5d-b7b0-be8c8eae6ed9",
   "metadata": {},
   "source": [
    "## Create city boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0dc7ca-d7e6-4a92-9fa5-c6e1a7d2c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_city_boundary(city_boundary_shp):\n",
    "    \"\"\"\n",
    "    create_city_boundary extracts a polygon outline of a city\n",
    "\n",
    "    :city_boundary_shp: shapefile containing polygon outline of city\n",
    "\n",
    "    :return: returns a polygon outlining the city in WGS84\n",
    "    \"\"\"\n",
    "    city_boundary_gdf = (\n",
    "        gpd.read_file(city_boundary_shp)\n",
    "        .to_crs('EPSG:4326') # project to classic CRS\n",
    "    )\n",
    "    \n",
    "    return city_boundary_gdf.geometry.iloc[0]\n",
    "\n",
    "city_boundary_shp = 'data/lfucg_usb/Urban_Service_Area.shp'\n",
    "\n",
    "city_boundary = create_city_boundary(city_boundary_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1e737-9598-4aa8-a6ba-f2788a28d767",
   "metadata": {},
   "source": [
    "## Create nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3525e-10f8-471c-8075-6589c6902082",
   "metadata": {},
   "source": [
    "### Origin nodes: census blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481c1669-4321-4b1d-bd0e-e465a8a81be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_origin_nodes_df(geodata_shp,pop_csv):\n",
    "    \"\"\"\n",
    "    create_origin_nodes_df creates a DataFrame of census blocks\n",
    "\n",
    "    :geodata_shp: shapefile from US Census Bureau\n",
    "    :pop_csv: population data from US Census Bureau\n",
    "    \n",
    "    :return: returns a DataFrame with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    \"\"\"\n",
    "    # census block geometry\n",
    "    census_blocks_gdf = (\n",
    "        gpd\n",
    "        .read_file(geodata_shp)\n",
    "        .query('GEOID20 not in @census_block_exclude_list') # remove blocks from network\n",
    "    )\n",
    "    # population data\n",
    "    population_df = (\n",
    "        pd\n",
    "        .read_csv(pop_csv)\n",
    "        .query('GEO_ID != \"Geography\"') # DECENNIALPL2020 has a \"header\" row\n",
    "        .drop_duplicates() # DECENNIALPL2020 has 101 rows that are listed twice \n",
    "    )\n",
    "    \n",
    "    # merge geo and pop data\n",
    "    df = (\n",
    "        population_df\n",
    "        .assign(\n",
    "            geoid20=lambda x: x['GEO_ID'].str.slice(9, 24), # reformat the geoid to match geoblocks_df\n",
    "            H1_001N=lambda x: x['H1_001N'].astype(int) # convert to integer\n",
    "        ) \n",
    "        \n",
    "        # merge with census blocks geographic data \n",
    "        .merge(census_blocks_gdf, left_on='geoid20', right_on='GEOID20', how='right') \n",
    "    \n",
    "        # create new columns\n",
    "        .assign(\n",
    "            name = lambda x: x['GEOID20'].str[-8:], # last eight digits of GEOID20\n",
    "            node_type = 'origin'\n",
    "        )\n",
    "    \n",
    "        # rename columns \n",
    "        .rename(columns={\n",
    "            'GEOID20' : 'id_string',\n",
    "            'H1_001N' : 'netflow', \n",
    "            'INTPTLAT20' : 'lat', \n",
    "            'INTPTLON20' : 'lon'\n",
    "        })\n",
    "    \n",
    "        # select nodes with nonzero population\n",
    "        .query(\"netflow > 0\")\n",
    "    \n",
    "        # select points inside USB\n",
    "        .assign(\n",
    "            # define a Point so we can apply .within()\n",
    "            geometry=lambda df: df.apply(\n",
    "                lambda x: Point(x['lon'], x['lat']) if (pd.notna(x['lon']) and pd.notna(x['lat'])) else None,\n",
    "                axis=1\n",
    "            ),\n",
    "            # define a truth column\n",
    "            in_USB=lambda df: df['geometry'].apply(lambda point: city_boundary.contains(point))\n",
    "        )\n",
    "        .query(\"in_USB == True\")\n",
    "    \n",
    "        # reset index — this removes skips in the index (needed for origin_to_intermediate arc creation)\n",
    "        .reset_index()\n",
    "        \n",
    "        # select columns\n",
    "        [['id_string','name','node_type','netflow','lat','lon']]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "geodata_shp = 'data/tl_2020_21067_tabblock20/tl_2020_21067_tabblock20.shp'\n",
    "pop_csv = 'data/DECENNIALPL2020.H1_2024-04-02T135509/DECENNIALPL2020.H1-Data.csv'\n",
    "\n",
    "origin_nodes_df = create_origin_nodes_df(geodata_shp,pop_csv)\n",
    "\n",
    "# calculate the total model population (for destination, sink netflows)\n",
    "total_pop = origin_nodes_df['netflow'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca99d0c-0bd6-4e08-879d-1ff08fd7ac98",
   "metadata": {},
   "source": [
    "### Intermediate nodes: cycle network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d9e04d-4656-4270-923b-9c3f0a3307b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intermediate_nodes_df(pfb_shp):\n",
    "    \"\"\"\n",
    "    create_intermediate_nodes_df creates a DataFrame of intermediate nodes\n",
    "\n",
    "    :pfb_shp: shapefile from PeopleForBikes, including segment stress \n",
    "    :city_boundary_shp: shapefile outlining the city boundary\n",
    "    \n",
    "    :return: returns a DataFrame with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    \"\"\"\n",
    "    # load PeopleForBikes cycle network data\n",
    "    pfb_gdf = (\n",
    "        gpd.read_file(pfb_shp)\n",
    "        .query('ROAD_ID not in @arc_exclude_list') # remove disconnected edges from the network\n",
    "        .query(\"FUNCTIONAL != 'motorway'\") # remove the highways\n",
    "        .query(\"FUNCTIONAL != 'motorway_link'\") # exit ramps etc.\n",
    "    )\n",
    "\n",
    "    # isolate arc heads\n",
    "    heads_identifiers_gdf = (\n",
    "        pfb_gdf\n",
    "        \n",
    "        # select columns\n",
    "        [['INTERSECTI','geometry']]\n",
    "        \n",
    "        # change data types, create new columns\n",
    "        .assign(\n",
    "            identifier = lambda x: x['INTERSECTI'].astype(int),\n",
    "            geometry = lambda x: (x['geometry'].to_crs(\"EPSG:4326\")), # convert to classic CRS\n",
    "            coord = lambda x: (x['geometry'].apply(\n",
    "                lambda line: (line.coords[0][1], line.coords[0][0]) if line else None) # grab first coord, switch order\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        # select points inside USB\n",
    "        .assign(\n",
    "            # define a truth column\n",
    "            in_USB=lambda df: df['geometry'].apply(lambda point: city_boundary.contains(point))\n",
    "        )\n",
    "        .query(\"in_USB == True\")\n",
    "    \n",
    "        # re-select columns\n",
    "        [['identifier','coord']]\n",
    "    )\n",
    "    \n",
    "    # isolate arc tails\n",
    "    tails_identifiers_gdf = (\n",
    "        pfb_gdf\n",
    "        \n",
    "        # select columns\n",
    "        [['INTERSE_01','geometry']]\n",
    "        \n",
    "        # change data types, create new columns\n",
    "        .assign(\n",
    "            identifier = lambda x: x['INTERSE_01'].astype(int),\n",
    "            geometry = lambda x: (x['geometry'].to_crs(\"EPSG:4326\")), # convert to classic CRS\n",
    "            coord = lambda x: (x['geometry'].apply(\n",
    "                lambda line: (line.coords[-1][1], line.coords[-1][0]) if line else None) # grab last coord, switch order\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        # select points inside USB\n",
    "        .assign(\n",
    "            # define a truth column\n",
    "            in_USB=lambda df: df['geometry'].apply(lambda point: city_boundary.contains(point))\n",
    "        )\n",
    "        .query(\"in_USB == True\")\n",
    "    \n",
    "        # re-select columns\n",
    "        [['identifier','coord']]\n",
    "    )\n",
    "    \n",
    "    # combine into single df\n",
    "    df = (\n",
    "        # concat\n",
    "        pd.concat([heads_identifiers_gdf, tails_identifiers_gdf], axis=0, ignore_index=True)\n",
    "    \n",
    "        # remove duplicates — this can cause skips in the index\n",
    "        .drop_duplicates()\n",
    "    \n",
    "        # reset index — this removes skips in the index\n",
    "        .reset_index()\n",
    "    \n",
    "        # create new columns\n",
    "        .assign(\n",
    "            name = '', # blank for now, we just need this column to match the amenity gdf\n",
    "            node_type = 'intermediate',\n",
    "            netflow = 0, \n",
    "            lat = lambda x: x['coord'].apply(lambda x: x[0] if x else None),\n",
    "            lon = lambda x: x['coord'].apply(lambda x: x[1] if x else None) \n",
    "        )\n",
    "    \n",
    "        # rename columns \n",
    "        .rename(columns={'identifier' : 'id_string'})\n",
    "    \n",
    "        # select columns\n",
    "        [['id_string','name','node_type','netflow','lat','lon']]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "pfb_shp = 'data/people_for_bikes/neighborhood_ways/neighborhood_ways.shp'\n",
    "\n",
    "intermediate_nodes_df = create_intermediate_nodes_df(pfb_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917c568-cfec-437f-9eb4-9c0caec82283",
   "metadata": {},
   "source": [
    "### Destination nodes: amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59ba50-81da-4bcd-9b1d-3a248e2bc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function \n",
    "def get_destination_nodes_df(amenities_geojson):\n",
    "    \"\"\"\n",
    "    get_destination_nodes_df creates a dataframe of destination nodes\n",
    "\n",
    "    :amenities_gdf: geojson of amenities\n",
    "    \n",
    "    :return: returns a dataframe with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    \"\"\"\n",
    "    # create amenities_gdf\n",
    "    gdf = (\n",
    "        gpd\n",
    "        .read_file(amenities_geojson)\n",
    "        #.cx[-84.6 : -84.3,  37.9 : 38.2] # filter non-KY Lexingtons using a bounding box # replace with USB filter later?\n",
    "        .query('id not in @dest_exclude_list') # remove individual entries\n",
    "    )\n",
    "\n",
    "    # open file from overpass turbo\n",
    "    df = (\n",
    "        amenities_gdf\n",
    "    \n",
    "        # create new columns\n",
    "        .assign(\n",
    "            node_type = 'destination',\n",
    "            rep_point = lambda x: x['geometry'].representative_point(), # define representative point\n",
    "            netflow = 0, \n",
    "            lat = lambda x: x['rep_point'].apply(lambda point: point.y if point else None), # extract lat\n",
    "            lon = lambda x: x['rep_point'].apply(lambda point: point.x if point else None) # extract lon\n",
    "        )\n",
    "    \n",
    "        # rename columns \n",
    "        .rename(columns={'id' : 'id_string'})\n",
    "    \n",
    "        # select points inside USB\n",
    "        .assign(\n",
    "            # define a truth column\n",
    "            in_USB=lambda df: df['geometry'].apply(lambda point: city_boundary.contains(point))\n",
    "        )\n",
    "        .query(\"in_USB == True\")\n",
    "    \n",
    "        # reset index — this removes skips in the index\n",
    "        .reset_index()\n",
    "    \n",
    "        # select columns\n",
    "        [['id_string','name','node_type','netflow','lat','lon']]\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# call the function\n",
    "amenities_geojson = 'data/lex_parks_export.geojson'\n",
    "\n",
    "dest_nodes_df = get_destination_nodes_df(amenities_geojson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d20287-c7e1-4171-b554-48b507855fc1",
   "metadata": {},
   "source": [
    "### Combine into a single DataFrame, create sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "5085ac86-4fdd-49d8-9315-3d20be80c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes_df(origin_nodes_df,intermediate_nodes_df,dest_nodes_df):\n",
    "    \"\"\"\n",
    "    create_nodes_df unifies the origin, intermediate, and destination nodes; creates a sink node\n",
    "\n",
    "    :origin_nodes_df: DataFrame of origins with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    :intermediate_nodes_df: DataFrame of intermediate nodes with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    :dest_nodes_df: a DataFrame of desintations with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    \n",
    "    :return: returns a DataFrame with columns 'id_string','name','node_type','netflow','lat','lon'\n",
    "    \"\"\"    \n",
    "    # create sink\n",
    "    new_row = gpd.GeoDataFrame(\n",
    "        {\n",
    "            'id_string': 'sink', # name it\n",
    "            'name': 'sink', \n",
    "            'node_type' : 'sink',\n",
    "            'netflow': -1*total_pop, # this balances the negative flows of the amenities\n",
    "            'lat': [0], # put it somewhere not in the network\n",
    "            'lon': [0] # put it somewhere not in the network\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # combine into single gdf\n",
    "    nodes_df = pd.concat([origin_nodes_df, intermediate_nodes_df, dest_nodes_df, new_row], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "nodes_df = create_nodes_df(origin_nodes_df,intermediate_nodes_df,dest_nodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fab84-8716-4b29-b86a-cdd50d46bc1c",
   "metadata": {},
   "source": [
    "## Create arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff349841-c0a4-4376-a7e4-c83b3eb7294c",
   "metadata": {},
   "source": [
    "### Make pairwise connector funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34aacdc-acd9-4cd3-9e6b-94eae9f09200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_arcs(tail_nodes_df,head_nodes_df,radii,projection):\n",
    "    \"\"\"\n",
    "    create_stratified_arcs makes (tail,head) arcs from disconnected tail nodes, shorter than each entry in the strata list\n",
    "    # there's gotta be a better way to describe this\n",
    "\n",
    "    :tail_nodes_df: a data frame of nodes to connect from, with columns 'id_string','lat','lon'\n",
    "    :head_nodes_df: a data frame of nodes to connect to, with columns 'id_string','lat','lon'\n",
    "    :radii: an increasing list of radii (in meters) to connect within\n",
    "    :projection: use a CRS suitable for pairwise distance calculation (e.g. 'ESRI:102003' for Albers contiguous USA) \n",
    "\n",
    "    :return: returns a DataFrame with arcs for rows \n",
    "    \"\"\"\n",
    "    # project tail nodes\n",
    "    tail_nodes_projected_gdf = (\n",
    "        # need a gdf\n",
    "        gpd.GeoDataFrame(\n",
    "            tail_nodes_df,  # original DataFrame\n",
    "            geometry=gpd.points_from_xy(tail_nodes_df['lon'], tail_nodes_df['lat']),  # create geometry column\n",
    "            crs='EPSG:4326'  # define the CRS (WGS84 for latitude/longitude)\n",
    "        )\n",
    "        \n",
    "        # project to USAC AUAC\n",
    "        .to_crs(projection)\n",
    "        \n",
    "        # make the index a column so we can reference it later\n",
    "        .reset_index() \n",
    "    )\n",
    "    \n",
    "    # project head nodes\n",
    "    head_nodes_projected_gdf = (\n",
    "        # need a gdf\n",
    "        gpd.GeoDataFrame(\n",
    "            head_nodes_df,  # original DataFrame\n",
    "            geometry=gpd.points_from_xy(head_nodes_df['lon'], head_nodes_df['lat']),\n",
    "            crs='EPSG:4326'  # define the CRS (WGS84 for latitude/longitude)\n",
    "        )\n",
    "        \n",
    "        # project to USAC AUAC\n",
    "        .to_crs(projection)\n",
    "        \n",
    "        # make the index a column so we can reference it later\n",
    "        .reset_index() \n",
    "    )\n",
    "    \n",
    "    # extract coordinates of geometries\n",
    "    tail_coords = np.array([geom.coords[0] for geom in tail_nodes_projected_gdf.geometry])\n",
    "    head_coords = np.array([geom.coords[0] for geom in head_nodes_projected_gdf.geometry])\n",
    "    \n",
    "    # compute pairwise distances\n",
    "    distance_matrix = cdist(tail_coords, head_coords) \n",
    "    \n",
    "    # identify (tail,head) pairs within stratified radii\n",
    "    # stack matrix into a DataFrame\n",
    "    distance_df = (\n",
    "        # convert matrix to df object\n",
    "        pd.DataFrame(\n",
    "            distance_matrix,\n",
    "            index = tail_nodes_projected_gdf.index,  # use tail indices\n",
    "            columns = head_nodes_projected_gdf.index  # use head indices\n",
    "        )\n",
    "        .stack() # change shape so distance is a column\n",
    "        .reset_index()  # unstack to have row-column pairs \n",
    "        .rename(columns={'level_0': 'row_index', 'level_1': 'col_index', 0: 'distance'})\n",
    "    )\n",
    "    \n",
    "    # initialize list to fill with connected pairs (origin, destination, radius)\n",
    "    # use list instead of DataFrame for the sake of memory\n",
    "    index_pairs_list = []\n",
    "    \n",
    "    for r in radii:\n",
    "        # identify index pairs with distance <= r\n",
    "        new_pairs_list = (\n",
    "            distance_df\n",
    "            .query('distance <= @r')  # filter distances \n",
    "            .assign(radius=r) # keep track of radius\n",
    "            .values.tolist() # convert to list\n",
    "        )\n",
    "        \n",
    "        # concatenate new index pairs and radius to index_pairs_list\n",
    "        index_pairs_list = index_pairs_list + new_pairs_list \n",
    "        \n",
    "        # determine which origin nodes have been connected\n",
    "        new_tails_list = list({tup[0] for tup in new_pairs_list})\n",
    "    \n",
    "        # update distance_df to exclude newly connected origin nodes\n",
    "        distance_df = distance_df[~distance_df['row_index'].isin(new_tails_list)]\n",
    "\n",
    "    # merge with original data\n",
    "    df = (\n",
    "    # convert to DataFrame\n",
    "        pd.DataFrame(\n",
    "            index_pairs_list, \n",
    "            columns=['row_index','col_index','distance','radius']\n",
    "        )\n",
    "        \n",
    "        # merge with original node data \n",
    "        .merge(tail_nodes_projected_gdf, left_on='row_index', right_on='index') # merge with tail data\n",
    "        .merge(head_nodes_projected_gdf, left_on='col_index', right_on='index') # merge with head data\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429ef78-8abc-4287-8381-21555a6f49aa",
   "metadata": {},
   "source": [
    "### Arcs from census blocks to bike network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce303fb-4ddf-453c-aa4b-4bbe02141891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_origin_to_intermediate_arcs(origin_nodes_df,intermediate_nodes_df):\n",
    "    \"\"\"\n",
    "    create_origin_to_intermediate_arcs makes arcs from origins to intermediate nodes\n",
    "    \n",
    "    :origin_nodes_df: origin nodes DataFrame\n",
    "    :intermediate_nodes_df: intermediate nodes DataFrame\n",
    "\n",
    "    :return: a GeoDataFrame of arcs\n",
    "    \"\"\"\n",
    "    # create edges\n",
    "    # call pairwise connector function\n",
    "    df = create_stratified_arcs(origin_nodes_df,intermediate_nodes_df,radii,projection)\n",
    "    \n",
    "    # construct a GeoDataFrame\n",
    "    gdf = (\n",
    "        gpd.GeoDataFrame(\n",
    "            df, # original DataFrame\n",
    "            geometry=origin_to_intermediate_arcs_df.apply(lambda row: LineString([row['geometry_x'], row['geometry_y']]), axis=1),\n",
    "            crs=projection # define the CRS \n",
    "        )\n",
    "    \n",
    "        # project back to classic CRS \n",
    "        .to_crs('EPSG:4326')\n",
    "        \n",
    "        # create new columns\n",
    "        .assign(\n",
    "            arc_type = 'origin_to_intermediate',\n",
    "            in_H = 0, # all low stress\n",
    "            dist = 0, # all 0 distance\n",
    "            tail_id = lambda x: x['id_string_x'],\n",
    "            head_id = lambda x: x['id_string_y']\n",
    "        )\n",
    "    \n",
    "        # rename columns \n",
    "        .rename(columns={\n",
    "            #'id_string_x' : 'tail_id',\n",
    "            'lat_x' : 'tail_lat',\n",
    "            'lon_x' : 'tail_lon',\n",
    "            #'id_string_y' : 'head_id',\n",
    "            'lat_y' : 'head_lat',\n",
    "            'lon_y' : 'head_lon',\n",
    "            #'radius' : 'cxn_radius'\n",
    "        })\n",
    "        \n",
    "        # select columns\n",
    "        [['tail_id','tail_lat','tail_lon','head_id','head_lat','head_lon','arc_type','in_H','dist','geometry']]\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "projection = distance_crs\n",
    "\n",
    "origin_to_intermediate_arcs_gdf = create_origin_to_intermediate_arcs(origin_nodes_df,intermediate_nodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf0781-236d-409d-a0c4-62ef85f903e4",
   "metadata": {},
   "source": [
    "### Bike network arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f2103-2a6f-4074-b7b1-6051f9cf15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intermediate_arcs(pfb_shp):\n",
    "    \"\"\"\n",
    "    create_intermediate_arcs reformats PeopleForBikes data to make arcs for the intermediate network\n",
    "    \n",
    "    :pfb_shp: shapefile from PeopleForBikes, including segment stress \n",
    "\n",
    "    :return: a GeoDataFrame of arcs\n",
    "    \"\"\"\n",
    "    # load PeopleForBikes cycle network data\n",
    "    pfb_gdf = (\n",
    "        gpd.read_file(pfb_shp)\n",
    "        .query('ROAD_ID not in @arc_exclude_list') # remove disconnected edges from the network\n",
    "        .query(\"FUNCTIONAL != 'motorway'\") # remove the highways\n",
    "        .query(\"FUNCTIONAL != 'motorway_link'\") # exit ramps etc.\n",
    "    )\n",
    "    \n",
    "    # gather from-to rows\n",
    "    pfb_ft_gdf = ( \n",
    "        pfb_gdf\n",
    "        \n",
    "        # select rows from PFB that have From-To data\n",
    "        .query('FT_SEG_STR.isnull() == False')\n",
    "        \n",
    "        # change data types, create new columns\n",
    "        .assign(\n",
    "            arc_type = 'intermediate',\n",
    "            geometry = lambda x: (x['geometry'].to_crs(\"EPSG:4326\")), # convert to classic CRS \n",
    "            tail_id = lambda x: x['INTERSECTI'],\n",
    "            tail_coord = lambda x: (x['geometry'].apply(\n",
    "                lambda x: (x.coords[0][1], x.coords[0][0]) if x else None) # grab first coord, switch order\n",
    "            ),\n",
    "            tail_lat = lambda x: x['tail_coord'].apply(lambda x: x[0] if x else None), \n",
    "            tail_lon = lambda x: x['tail_coord'].apply(lambda x: x[1] if x else None),\n",
    "            head_id = lambda x: x['INTERSE_01'],\n",
    "            head_coord = lambda x: (x['geometry'].apply(\n",
    "                lambda x: (x.coords[-1][1], x.coords[-1][0]) if x else None) # grab last coord, switch order\n",
    "            ),\n",
    "            head_lat = lambda x: x['head_coord'].apply(lambda x: x[0] if x else None),\n",
    "            head_lon = lambda x: x['head_coord'].apply(lambda x: x[1] if x else None),\n",
    "            in_H = lambda x: np.where(x['FT_SEG_STR'] == 1, 0, 1), # if pfb stress == 1, then not in H, else in H\n",
    "            dist = (\n",
    "                pfb_gdf.to_crs(\"ESRI:102003\") # project for distance calculation\n",
    "                .geometry.length\n",
    "            ),\n",
    "            in_USB=lambda df: df['geometry'].apply(lambda point: city_boundary.contains(point))\n",
    "        )\n",
    "    \n",
    "        # select columns\n",
    "        [['tail_id','tail_lat','tail_lon','head_id','head_lat','head_lon','arc_type','in_H','in_H2','dist','geometry']]\n",
    "    )\n",
    "    \n",
    "    # gather to-from rows\n",
    "    pfb_tf_gdf = (\n",
    "        pfb_gdf\n",
    "        \n",
    "        # select rows from PFB that have To-From data\n",
    "        .query('TF_SEG_STR.isnull() == False')\n",
    "        \n",
    "        # change data types, create new columns\n",
    "        .assign(\n",
    "            arc_type = 'intermediate',\n",
    "            geometry = lambda x: (\n",
    "                x['geometry']\n",
    "                .to_crs(\"EPSG:4326\") # convert to classic CRS\n",
    "                .reverse() # reverse order of LineString since To-From is backwards \n",
    "            ),   \n",
    "            tail_id = lambda x: x['INTERSE_01'],\n",
    "            tail_coord = lambda x: (x['geometry'].apply(\n",
    "                lambda x: (x.coords[0][1], x.coords[0][0]) if x else None) # grab first coord, switch order\n",
    "            ),\n",
    "            tail_lat = lambda x: x['tail_coord'].apply(lambda x: x[0] if x else None), \n",
    "            tail_lon = lambda x: x['tail_coord'].apply(lambda x: x[1] if x else None),\n",
    "            head_id = lambda x: x['INTERSECTI'],\n",
    "            head_coord = lambda x: (x['geometry'].apply(\n",
    "                lambda x: (x.coords[-1][1], x.coords[-1][0]) if x else None) # grab last coord, switch order\n",
    "            ),\n",
    "            head_lat = lambda x: x['head_coord'].apply(lambda x: x[0] if x else None),\n",
    "            head_lon = lambda x: x['head_coord'].apply(lambda x: x[1] if x else None),\n",
    "            in_H = lambda x: np.where(x['TF_SEG_STR'] == 1, 0, 1), # if pfb stress == 1, then not in H, else in H\n",
    "            dist = (\n",
    "                pfb_gdf.to_crs(\"ESRI:102003\") # project for distance calculation\n",
    "                .geometry.length\n",
    "            ),\n",
    "            in_USB=lambda df: df['geometry'].apply(lambda point: city_boundary.contains(point))\n",
    "        )\n",
    "        \n",
    "        # select columns\n",
    "        [['tail_id','tail_lat','tail_lon','head_id','head_lat','head_lon','arc_type','in_H','dist','geometry']]\n",
    "    )\n",
    "    \n",
    "    # create a list of bike network nodes \n",
    "    intermediate_id_list = intermediate_nodes_df['id_string'].tolist()\n",
    "    \n",
    "    # combine From-To and To-From\n",
    "    intermediate_arcs_gdf = (\n",
    "        pd.concat([pfb_ft_gdf, pfb_tf_gdf], axis=0, ignore_index=True)\n",
    "        # only keep arcs that are incident to nodes in the USB\n",
    "        .query(\"tail_id in @intermediate_id_list\")\n",
    "        .query(\"head_id in @intermediate_id_list\")\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "pfb_shp = 'data/people_for_bikes/neighborhood_ways/neighborhood_ways.shp'\n",
    "\n",
    "intermediate_arcs_gdf = create_intermediate_arcs(pfb_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeed1e0-15cd-4737-98b2-f569b786e66d",
   "metadata": {},
   "source": [
    "### Arcs from bike network to amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9878d-a2c1-4598-ac36-e6a571b36baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intermediate_to_dest_arcs(intermediate_nodes_df,dest_nodes_df):\n",
    "    \"\"\"\n",
    "    create_intermediate_to_dest_arcs makes arcs from intermediate nodes to destinations\n",
    "    \n",
    "    :intermediate_nodes_df: intermediate nodes DataFrame \n",
    "    :dest_nodes_df: desination nodes DataFrame\n",
    "\n",
    "    :return: a GeoDataFrame of arcs\n",
    "    \"\"\"\n",
    "    # create edges\n",
    "    # call pairwise connector function\n",
    "    df = create_stratified_arcs(intermediate_nodes_df,dest_nodes_df,radii,projection)\n",
    "    \n",
    "    # construct a GeoDataFrame\n",
    "    gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        df, # original DataFrame\n",
    "        geometry=intermediate_to_dest_arcs_df.apply(lambda row: LineString([row['geometry_x'], row['geometry_y']]), axis=1),\n",
    "        crs=projection # define the CRS \n",
    "    )\n",
    "\n",
    "    # project back to classic CRS \n",
    "    .to_crs('EPSG:4326')\n",
    "    \n",
    "    # create new columns\n",
    "    .assign(\n",
    "        arc_type = 'intermediate_to_destination',\n",
    "        in_H = 0, # all low stress\n",
    "        dist = 0, # all 0 distance\n",
    "        tail_id = lambda x: x['id_string_x'],\n",
    "        head_id = lambda x: x['id_string_y'],\n",
    "    )\n",
    "\n",
    "    # rename columns \n",
    "    .rename(columns={\n",
    "        #'id_string_x' : 'tail_id',\n",
    "        'lat_x' : 'tail_lat',\n",
    "        'lon_x' : 'tail_lon',\n",
    "        #'id_string_y' : 'head_id',\n",
    "        'lat_y' : 'head_lat',\n",
    "        'lon_y' : 'head_lon',\n",
    "        #'radius' : 'cxn_radius'\n",
    "    })\n",
    "    \n",
    "    # select columns\n",
    "    [['tail_id','tail_lat','tail_lon','head_id','head_lat','head_lon','arc_type','in_H','dist','geometry']]\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "projection = distance_crs\n",
    "\n",
    "intermediate_to_dest_arcs_gdf = create_intermediate_to_dest_arcs(intermediate_nodes_df,dest_nodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18a025-ff6b-40ad-ab9e-710c9575bf53",
   "metadata": {},
   "source": [
    "### Arcs from amenities to sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d4d49-7062-4e65-b62c-422df63f2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dest_to_sink_arcs(dest_nodes_df):\n",
    "    \"\"\"\n",
    "    create_dest_to_sink_arcs makes an arc from each destination to a sink node\n",
    "\n",
    "    :dest_nodes_df: desination nodes DataFrame\n",
    "\n",
    "    :return: a GeoDataFrame of arcs\n",
    "    \"\"\"\n",
    "    gdf = (\n",
    "        # initialize gdf\n",
    "        gpd.GeoDataFrame(\n",
    "            dest_nodes_df,  # original DataFrame\n",
    "            geometry=dest_nodes_df.apply(lambda row: LineString([(row['lon'], row['lat']), (0, 0)]), axis=1),\n",
    "            crs='EPSG:4326'  # define the CRS (WGS84 for latitude/longitude)\n",
    "        )\n",
    "        # rename columns\n",
    "        .rename(columns={\n",
    "            #'id_string' : 'tail_id',\n",
    "            'lat' : 'tail_lat', \n",
    "            'lon' : 'tail_lon'\n",
    "        })\n",
    "    \n",
    "        # create columns\n",
    "        .assign(\n",
    "            tail_id = lambda x: x['id_string'],\n",
    "            head_id = 'sink',\n",
    "            head_lat = 0,\n",
    "            head_lon = 0,\n",
    "            arc_type = 'amenity_to_sink',\n",
    "            in_H = 0, # all low stress\n",
    "            dist = 0 # all 0 distance\n",
    "        )\n",
    "    \n",
    "        # select columns\n",
    "        [['tail_id','tail_lat','tail_lon','head_id','head_lat','head_lon','arc_type','in_H','dist','geometry']]\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "dest_to_sink_arcs_gdf = create_dest_to_sink_arcs(dest_nodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326177a-47aa-4e50-bbc0-21a486a95dcc",
   "metadata": {},
   "source": [
    "### Combine into a single GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae8eff-f4a5-46c8-abc9-d45cb4b5cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arcs_gdf(oi_arcs,i_arcs,id_arcs,ds_arcs):\n",
    "    \"\"\"\n",
    "    create_arcs_gdf unifies the separately created arcs\n",
    "\n",
    "    :oi_arcs: origin to intermediate arcs\n",
    "    :i_arcs: intermediate arcs\n",
    "    :id_arcs: intermediate to destination arcs\n",
    "    :ds_arcs: destination to sink arcs\n",
    "    \n",
    "    :return: a GeoDataFrame of arcs\n",
    "    \"\"\"\n",
    "    # concatenate returns a DataFrame\n",
    "    df = (\n",
    "        pd.concat(\n",
    "        [oi_arcs,i_arcs,id_arcs,ds_arcs],\n",
    "        ignore_index=True\n",
    "        )\n",
    "        .assign(arc_id = lambda x: list(zip(x['tail_id'],x['head_id']))) # creates a (sort of) unique arc id column\n",
    "        [['arc_id','tail_id','tail_lat','tail_lon','head_id','head_lat','head_lon','arc_type','in_H','dist','geometry']]\n",
    "    )\n",
    "\n",
    "    # convert to a GeoDataFrame\n",
    "    gdf = (\n",
    "        gpd.GeoDataFrame(\n",
    "            df, # original DataFrame\n",
    "            geometry=df['geometry'],\n",
    "            crs='EPSG:4326' # define the CRS \n",
    "        )\n",
    "\n",
    "    return gdf\n",
    "\n",
    "oi_arcs=origin_to_intermediate_arcs_gdf\n",
    "i_arcs=intermediate_arcs_gdf\n",
    "id_arcsintermediate_to_dest_arcs_gdf\n",
    "ds_arcs=dest_to_sink_arcs_gdf\n",
    "\n",
    "arcs_gdf = create_arcs_gdf(oi_arcs,i_arcs,id_arcs,ds_arcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8be537-c50f-476b-a3ce-28f8ac40f95f",
   "metadata": {},
   "source": [
    "## Create solver objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b02514-c6d0-44d6-9b4c-d0d63184b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_solver_objects():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c55a60-39f5-4e2a-9d88-e735736ba238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node-related lists\n",
    "nodes = nodes_df['id_string'].tolist()\n",
    "node_to_netflow = (\n",
    "    nodes_df\n",
    "    .set_index('id_string')['netflow']  # use id_string as keys, netflow as values\n",
    "    .to_dict()  # convert to dictionary\n",
    ")\n",
    "\n",
    "# arc-related lists\n",
    "arcs = arcs_gdf['arc_id'].tolist()\n",
    "high_stress_arcs = arcs_gdf.query(\"in_H==1\")['arc_id'].tolist()\n",
    "\n",
    "arc_to_dist = (\n",
    "    arcs_gdf\n",
    "    .set_index('arc_id')['dist']  # use id_string as keys, dist as values\n",
    "    .to_dict()  # convert to dictionary\n",
    ")\n",
    "\n",
    "high_stress_arc_to_cost = (\n",
    "    arcs_gdf\n",
    "    .query(\"in_H==1\")\n",
    "    .assign(cost=lambda x: cost_per_meter * x['dist'])\n",
    "    .set_index(['tail_id', 'head_id'])['cost']  # use (tail_id, head_id) as keys\n",
    "    .to_dict()  # convert to dictionary\n",
    ")\n",
    "\n",
    "# dictionaries of incident edges\n",
    "node_to_incoming_arcs = defaultdict(set)\n",
    "node_to_outgoing_arcs = defaultdict(set)\n",
    "\n",
    "for x,y in arcs_gdf.arc_id: \n",
    "    node_to_incoming_arcs[y].add((x,y))\n",
    "    node_to_outgoing_arcs[x].add((x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2332b83-eff8-499c-bec9-a1bfe86b911e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Main solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a411b-87c6-4940-953c-a5bf543c59c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define budget\n",
    "budget = total_cost_proportion * sum(high_stress_arc_to_cost.values())\n",
    "\n",
    "# set up the model\n",
    "model = pyo.ConcreteModel()\n",
    "\n",
    "# variables\n",
    "model.x = pyo.Var(arcs, domain=pyo.NonNegativeIntegers)\n",
    "model.y = pyo.Var(high_stress_arcs, domain=pyo.Binary)\n",
    "model.z = pyo.Var(high_stress_arcs, domain=pyo.Binary)\n",
    "\n",
    "# objective function\n",
    "def obj_rule(model):\n",
    "    return (sum(arc_to_dist[i,j]*model.x[i,j] for i,j in arcs) \n",
    "            + sum((f-1)*arc_to_dist[i,j]*model.z[i,j] for i,j in high_stress_arcs)\n",
    "           )\n",
    "    \n",
    "model.obj = pyo.Objective(rule=obj_rule,sense=pyo.minimize)\n",
    "\n",
    "# constraints\n",
    "## flow balance\n",
    "def flow_balance_rule(model,node):\n",
    "    return (\n",
    "        sum(model.x[node,j] for node,j in node_to_outgoing_arcs[node]) - sum(model.x[i,node] for i,node in node_to_incoming_arcs[node]) == node_to_netflow[node]\n",
    "    )\n",
    "\n",
    "model.flow_balance = pyo.Constraint(nodes,rule=flow_balance_rule)\n",
    "\n",
    "## use only low stress arcs or upgraded high stress arcs, or incur a penalty\n",
    "def low_stress_rule(model,i,j): \n",
    "    return model.z[i,j] >= model.x[i,j] - total_pop*model.y[i,j]  \n",
    "\n",
    "model.low_stress = pyo.Constraint(high_stress_arcs,rule=low_stress_rule)\n",
    "\n",
    "## upgrade bidirectional high stress arcs in pairs\n",
    "## temporarily suspending this constraint\n",
    "#def bidirectional_upgrade_rule(model,i,j):\n",
    "#    return model.y[i,j] <= model.y[j,i]\n",
    "\n",
    "#model.bidirectional_upgrade = pyo.Constraint(bidi_high_stress_arcs,rule=bidirectional_upgrade_rule)\n",
    "\n",
    "## stay on budget\n",
    "def budget_rule(model): \n",
    "    return sum(high_stress_arc_to_cost[i,j]*model.y[i,j] for i,j in high_stress_arcs) <= budget \n",
    "\n",
    "model.budget = pyo.Constraint(rule=budget_rule)\n",
    "\n",
    "# run the solver for the stress-free model\n",
    "solver_name = 'gurobi'\n",
    "solver = pyo.SolverFactory(solver_name)\n",
    "solver.options['TimeLimit'] = 60\n",
    "solver.options['MIPGap'] = 0.01\n",
    "results = solver.solve(model, tee=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fd9b2-f546-43c5-94e4-b2dd4975091e",
   "metadata": {},
   "source": [
    "# Analyze Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53016de8-569f-4d58-8433-aa4518437c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which edges were upgraded?\n",
    "# what was the objective value?\n",
    "# create maps etc.\n",
    "# color-weight upgraded arcs by how many people use them\n",
    "# color-code census blocks by destination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
